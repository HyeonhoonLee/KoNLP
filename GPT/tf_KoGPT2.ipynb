{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf_KoGPT2.ipynb",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "https://github.com/HyeonhoonLee/KoNLP/blob/master/GPT/tf_KoGPT2.ipynb",
      "authorship_tag": "ABX9TyMR5NAPTojWJ4eVA+qtpIKO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HyeonhoonLee/KoNLP/blob/master/GPT/tf_KoGPT2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZcQqI2rpoTQ"
      },
      "source": [
        "# Korean Language Modeling using GPT2\n",
        "\n",
        "- The contents of this notebook is modified from [NLP-Kr](https://github.com/NLP-kr/tensorflow-ml-nlp-tf2)\n",
        "- The source codes are based on [HuggingFace Transformers](https://github.com/huggingface/transformers) and [SKT KoGPT2](https://github.com/SKT-AI/KoGPT2)\n",
        "- To understand the pricinpale of GPT2, see this kindful page of [illustrated GPT-2](https://jalammar.github.io/illustrated-gpt2/).\n",
        "- The codes below use Tensorflow 2.3.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bR1lM7RrNeU"
      },
      "source": [
        "##Libraries and modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpaMiYBExfq7"
      },
      "source": [
        "!pip install gluonnlp\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yu2u-6RTxtRZ"
      },
      "source": [
        "!pip install mxnet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ITq77Fs1kXe"
      },
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPBDCbMAoEtE"
      },
      "source": [
        "# To use the word vocab API (https://nlp.gluon.ai/api/vocab.html).\n",
        "import gluonnlp as nlp  \n",
        "\n",
        "# Do not use the tokenizer of Transformers. KoGPT2 model used this type of Tokenizer.\n",
        "# This tokenizer supports subword tokenization such as BPE(Byte pair Encoding).\n",
        "# (https://nlp.gluon.ai/api/data.html)\n",
        "from gluonnlp.data import SentencepieceTokenizer\n",
        "\n",
        "# To use Open AI GPT2 Model transformer with a language modeling head on top (linear layer with weights tied to the input embeddings).\n",
        "# (https://huggingface.co/transformers/model_doc/gpt2.html#gpt2lmheadmodel)\n",
        "from transformers import TFGPT2LMHeadModel"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLr1_hb502vN"
      },
      "source": [
        "##Korean GPT2 Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-F_KB-h2LaQ"
      },
      "source": [
        "# # Download the pretrained parameters.\n",
        "# !wget https://github.com/NLP-kr/tensorflow-ml-nlp-tf2/releases/download/v1.0/gpt_ckpt.zip -O gpt_ckpt.zip\n",
        "# !unzip -o gpt_ckpt.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yz-KD16xduQ"
      },
      "source": [
        "# To define the GPT2Model class.\n",
        "class GPT2Model(tf.keras.Model):\n",
        "  def __init__(self, dir_path):\n",
        "    super(GPT2Model, self).__init__()\n",
        "    self.gpt2 = TFGPT2LMHeadModel.from_pretrained(dir_path)\n",
        "  \n",
        "  # object gpt2 includes 4 outputs of tuple (last_hidden_states, past, hidden_state, attentions)\n",
        "  def call(self, inputs):\n",
        "    return self.gpt2(inputs)[0]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ea5pRuNd2oag",
        "outputId": "ec024061-fdea-47af-ea65-2d8761f7cac2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# We used Colab.\n",
        "BASE_MODEL_PATH = '/content/drive/My Drive/ModelCollection/gpt_ckpt/'\n",
        "# Create pre-trained gpt2 model.\n",
        "gpt_model = GPT2Model(BASE_MODEL_PATH)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at /content/drive/My Drive/ModelCollection/gpt_ckpt/.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9tEvF8x3P2f"
      },
      "source": [
        "## Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdSZelX224q8"
      },
      "source": [
        "TOKENIZER_PATH = os.path.join(BASE_MODEL_PATH + 'gpt2_kor_tokenizer.spiece')\n",
        "\n",
        "# To make tokenizer for KoGPT2.\n",
        "tokenizer = SentencepieceTokenizer(TOKENIZER_PATH)\n",
        "\n",
        "# To define the word dictionary.\n",
        "vocab = nlp.vocab.BERTVocab.from_sentencepiece(TOKENIZER_PATH,\n",
        "                                               mask_token=None,\n",
        "                                               sep_token=None,\n",
        "                                               cls_token=None,\n",
        "                                               unknown_token='<unk>',\n",
        "                                               padding_token='<pad>',\n",
        "                                               bos_token='<s>',\n",
        "                                               eos_token='</s>')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-EshctH4U9A"
      },
      "source": [
        "## Sentence Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2osQt10N3i_7"
      },
      "source": [
        "def tf_top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-99999):\n",
        "    _logits = logits.numpy()\n",
        "    top_k = min(top_k, logits.shape[-1])  \n",
        "    \n",
        "    # top-k sampling method\n",
        "    if top_k > 0:\n",
        "        indices_to_remove = logits < tf.math.top_k(logits, top_k)[0][..., -1, None]\n",
        "        _logits[indices_to_remove] = filter_value\n",
        "    \n",
        "    # Nuclus sampling method\n",
        "    if top_p > 0.0:\n",
        "        sorted_logits = tf.sort(logits, direction='DESCENDING')\n",
        "        sorted_indices = tf.argsort(logits, direction='DESCENDING')\n",
        "        cumulative_probs = tf.math.cumsum(tf.nn.softmax(sorted_logits, axis=-1), axis=-1)\n",
        "\n",
        "        sorted_indices_to_remove = cumulative_probs > top_p\n",
        "        sorted_indices_to_remove = tf.concat([[False], sorted_indices_to_remove[..., :-1]], axis=0)\n",
        "        indices_to_remove = sorted_indices[sorted_indices_to_remove].numpy().tolist()\n",
        "        \n",
        "        _logits[indices_to_remove] = filter_value\n",
        "    return tf.constant([_logits])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2_17rB94X82"
      },
      "source": [
        "def generate_sent(seed_word, model, max_step=100, greedy=False, top_k=0, top_p=0.):\n",
        "    sent = seed_word   # Input sentence or word\n",
        "    toked = tokenizer(sent)  # Tokenizing\n",
        "    \n",
        "    for _ in range(max_step):  # max_step is the maximum size of sentence generating\n",
        "        input_ids = tf.constant([vocab[vocab.bos_token],]  + vocab[toked])[None, :] \n",
        "        outputs = model(input_ids)[:, -1, :] # Output is the last subword of sentence.\n",
        "        if greedy:  # greedy search with Maximum Likelihood Estimation.\n",
        "            gen = vocab.to_tokens(tf.argmax(outputs, axis=-1).numpy().tolist()[0])\n",
        "        else:    # Using top_k & Nucleus sampling.\n",
        "            output_logit = tf_top_k_top_p_filtering(outputs[0], top_k=top_k, top_p=top_p)\n",
        "            gen = vocab.to_tokens(tf.random.categorical(output_logit, 1).numpy().tolist()[0])[0]\n",
        "        if gen == '</s>': # Stop generating when meeting this special token.\n",
        "            break\n",
        "        sent += gen.replace('▁', ' ')\n",
        "        toked = tokenizer(sent)\n",
        "\n",
        "    return sent"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KomPy7uX71az"
      },
      "source": [
        "## Practice of sentence generating"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cynj79PC4Z9a",
        "outputId": "2b1931e5-6502-4a4d-a51f-d5088721c4d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "generate_sent('의학', gpt_model, greedy=True)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'의학전문대학원, 의과대학원과 같은 학원을 설립할 수 있는가에 대한 것이다.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRx1WS-27-2_",
        "outputId": "439271b5-3a4c-4b17-a8a3-27f0818a19e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "generate_sent('의학', gpt_model, top_k=0, top_p=0.95)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'의학, 흡연, 신장 연구의학을 전문적으로 하면서이지.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRDHwwKq8SiP"
      },
      "source": [
        "## Fine tuning (Korean novel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Blt7oSWJ8Voj"
      },
      "source": [
        "### Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFt3iK3YFHZG"
      },
      "source": [
        "#Hyperparameters\n",
        "BATCH_SIZE = 16\n",
        "NUM_EPOCHS = 10\n",
        "MAX_LEN = 30"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGaQPI8SDrbW",
        "outputId": "6d2b3dc8-399a-4fae-f2c3-b215340d2443",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Download text file (Korean novel)\n",
        "://raw.githubusercontent.com/NLP-kr/tensorflow-ml-nlp-tf2/master/7.PRETRAIN_METHOD/data_in/KOR/finetune_data.txt"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-03 07:42:38--  https://raw.githubusercontent.com/NLP-kr/tensorflow-ml-nlp-tf2/master/7.PRETRAIN_METHOD/data_in/KOR/finetune_data.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 24570 (24K) [text/plain]\n",
            "Saving to: ‘finetune_data.txt’\n",
            "\n",
            "finetune_data.txt   100%[===================>]  23.99K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2020-11-03 07:42:38 (1.80 MB/s) - ‘finetune_data.txt’ saved [24570/24570]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocjEOxsHAM48",
        "outputId": "39da8a5d-0e35-4fdd-f2f4-6dd58e72c297",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "TEXT_DATA_PATH = '/content/drive/My Drive/DataCollection/NLP/finetune_data.txt'\n",
        "\n",
        "sents = [s[:-1] for s in open(TEXT_DATA_PATH).readlines()]\n",
        "\n",
        "print('total number of sents :',len(sents))\n",
        "print('1st sents :',sents[0])"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total number of sents : 284\n",
            "1st sents : 그때에 김첨지는 대수롭지 않은듯이,\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9Wc5-9iEFN_",
        "outputId": "183a4e4c-9cf1-453a-b0ce-6c9cdadf96e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Tokenizing the text data and making input_data and output_data with special tokens.\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "input_data = []\n",
        "output_data = []\n",
        "\n",
        "for s in sents:\n",
        "    tokens = [vocab[vocab.bos_token],]  + vocab[tokenizer(s)] + [vocab[vocab.eos_token],]\n",
        "    input_data.append(tokens[:-1])\n",
        "    output_data.append(tokens[1:])\n",
        "\n",
        "input_data = pad_sequences(input_data, MAX_LEN, value=vocab[vocab.padding_token]) # default: padding='pre'\n",
        "output_data = pad_sequences(output_data, MAX_LEN, value=vocab[vocab.padding_token])\n",
        "\n",
        "input_data = np.array(input_data, dtype=np.int64)\n",
        "output_data = np.array(output_data, dtype=np.int64)\n",
        "\n",
        "print(sents[0])\n",
        "print(input_data[0])\n",
        "print(output_data[0])"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "그때에 김첨지는 대수롭지 않은듯이,\n",
            "[    3     3     3     3     3     3     3     3     3     0 47437 47522\n",
            " 47675 47442 47437 47633 48120 47445 47441 47437 47455 47467 48139 47445\n",
            " 47437 47676 47459 48090 47438 47453]\n",
            "[    3     3     3     3     3     3     3     3     3 47437 47522 47675\n",
            " 47442 47437 47633 48120 47445 47441 47437 47455 47467 48139 47445 47437\n",
            " 47676 47459 48090 47438 47453     1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-W3KCJ_8XsP"
      },
      "source": [
        "### Fine tuning with pre-trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zj2GSKbF8cTA"
      },
      "source": [
        "# Loss function and metric(accuracy)\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, vocab[vocab.padding_token]))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_)\n",
        "\n",
        "def accuracy_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, vocab[vocab.padding_token]))\n",
        "    mask = tf.expand_dims(tf.cast(mask, dtype=pred.dtype), axis=-1)\n",
        "    pred *= mask    \n",
        "    acc = train_accuracy(real, pred)\n",
        "\n",
        "    return tf.reduce_mean(acc)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNiAE3m3H0Bc"
      },
      "source": [
        "# compile model\n",
        "gpt_model.compile(loss=loss_function,\n",
        "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "              metrics=[accuracy_function])"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFoGzcB-H1Rn",
        "outputId": "3f4184ae-3667-4be9-c040-6874f22f771a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# train model...\n",
        "history = gpt_model.fit(input_data, output_data, \n",
        "                    batch_size=BATCH_SIZE, epochs=NUM_EPOCHS,\n",
        "                    validation_split=0.1)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "16/16 [==============================] - 2s 111ms/step - loss: 0.9697 - accuracy_function: 0.4256 - val_loss: 2.6411 - val_accuracy_function: 0.4319\n",
            "Epoch 2/10\n",
            "16/16 [==============================] - 2s 110ms/step - loss: 0.8450 - accuracy_function: 0.4395 - val_loss: 2.7476 - val_accuracy_function: 0.4455\n",
            "Epoch 3/10\n",
            "16/16 [==============================] - 2s 110ms/step - loss: 0.7437 - accuracy_function: 0.4529 - val_loss: 2.7952 - val_accuracy_function: 0.4589\n",
            "Epoch 4/10\n",
            "16/16 [==============================] - 2s 110ms/step - loss: 0.6574 - accuracy_function: 0.4662 - val_loss: 2.8630 - val_accuracy_function: 0.4718\n",
            "Epoch 5/10\n",
            "16/16 [==============================] - 2s 111ms/step - loss: 0.5825 - accuracy_function: 0.4790 - val_loss: 2.9604 - val_accuracy_function: 0.4841\n",
            "Epoch 6/10\n",
            "16/16 [==============================] - 2s 110ms/step - loss: 0.5167 - accuracy_function: 0.4908 - val_loss: 3.0062 - val_accuracy_function: 0.4956\n",
            "Epoch 7/10\n",
            "16/16 [==============================] - 2s 110ms/step - loss: 0.4638 - accuracy_function: 0.5016 - val_loss: 3.0226 - val_accuracy_function: 0.5066\n",
            "Epoch 8/10\n",
            "16/16 [==============================] - 2s 110ms/step - loss: 0.4152 - accuracy_function: 0.5126 - val_loss: 3.0891 - val_accuracy_function: 0.5169\n",
            "Epoch 9/10\n",
            "16/16 [==============================] - 2s 111ms/step - loss: 0.3833 - accuracy_function: 0.5224 - val_loss: 3.1445 - val_accuracy_function: 0.5265\n",
            "Epoch 10/10\n",
            "16/16 [==============================] - 2s 111ms/step - loss: 0.3565 - accuracy_function: 0.5318 - val_loss: 3.1044 - val_accuracy_function: 0.5355\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaBMwjnsH6D-"
      },
      "source": [
        "# Save the model\n",
        "DATA_OUT_PATH = '/content/drive/My Drive/DataCollection/NLP'\n",
        "model_name = \"tf2_gpt2_finetuned_model\"\n",
        "\n",
        "save_path = os.path.join(DATA_OUT_PATH, model_name)\n",
        "\n",
        "if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)\n",
        "\n",
        "gpt_model.gpt2.save_pretrained(save_path)"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcYDmXadIBAm",
        "outputId": "10edb3ff-7a71-4bdc-ee7b-1f7266a66263",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Load the model\n",
        "loaded_gpt_model = GPT2Model(save_path)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at /content/drive/My Drive/DataCollection/NLP/tf2_gpt2_finetuned_model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smR7-3h_JaW-"
      },
      "source": [
        "### Practice with fine-tuned model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qL6_hEnnICwy",
        "outputId": "137ae5eb-6d62-42c9-bd97-68c493647627",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "generate_sent('김첨지', gpt_model, greedy=True)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'김첨지는                                                                                                   '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itFUwSzfIG9S",
        "outputId": "730f52c2-b3e6-4462-93df-a303d88d0659",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "generate_sent('김첨지', gpt_model, top_k=3, top_p=0.0)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'김첨지의                                                                                                   '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uQHoj3oKU4V"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}