{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf_KoGPT2.ipynb",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "https://github.com/HyeonhoonLee/KoNLP/blob/master/GPT/tf_KoGPT2.ipynb",
      "authorship_tag": "ABX9TyPa+jaDSQQA5CEG4XkInPDf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HyeonhoonLee/KoNLP/blob/master/GPT/tf_KoGPT2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZcQqI2rpoTQ"
      },
      "source": [
        "# Korean Language Modeling using GPT2\n",
        "\n",
        "- The contents of this notebook is modified from [NLP-Kr](https://github.com/NLP-kr/tensorflow-ml-nlp-tf2)\n",
        "- The source codes are based on [HuggingFace Transformers](https://github.com/huggingface/transformers) and [SKT KoGPT2](https://github.com/SKT-AI/KoGPT2)\n",
        "- To understand the pricinpale of GPT2, see this kindful page of [illustrated GPT-2](https://jalammar.github.io/illustrated-gpt2/).\n",
        "- The codes below use Tensorflow 2.3.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bR1lM7RrNeU"
      },
      "source": [
        "##Libraries and modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpaMiYBExfq7"
      },
      "source": [
        "!pip install gluonnlp\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yu2u-6RTxtRZ"
      },
      "source": [
        "!pip install mxnet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ITq77Fs1kXe"
      },
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPBDCbMAoEtE"
      },
      "source": [
        "# To use the word vocab API (https://nlp.gluon.ai/api/vocab.html).\n",
        "import gluonnlp as nlp  \n",
        "\n",
        "# Do not use the tokenizer of Transformers. KoGPT2 model used this type of Tokenizer.\n",
        "# This tokenizer supports subword tokenization such as BPE(Byte pair Encoding).\n",
        "# (https://nlp.gluon.ai/api/data.html)\n",
        "from gluonnlp.data import SentencepieceTokenizer\n",
        "\n",
        "# To use Open AI GPT2 Model transformer with a language modeling head on top (linear layer with weights tied to the input embeddings).\n",
        "# (https://huggingface.co/transformers/model_doc/gpt2.html#gpt2lmheadmodel)\n",
        "from transformers import TFGPT2LMHeadModel"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLr1_hb502vN"
      },
      "source": [
        "##Korean GPT2 Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-F_KB-h2LaQ",
        "outputId": "456f74e9-ec60-43b2-cdff-3f077deef6ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!wget https://www.dropbox.com/s/nzfa9xpzm4edp6o/gpt_ckpt.zip -O gpt_ckpt.zip\n",
        "!unzip -o gpt_ckpt.zip"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-03 06:43:36--  https://www.dropbox.com/s/nzfa9xpzm4edp6o/gpt_ckpt.zip\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.1, 2620:100:6018:1::a27d:301\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/nzfa9xpzm4edp6o/gpt_ckpt.zip [following]\n",
            "--2020-11-03 06:43:36--  https://www.dropbox.com/s/raw/nzfa9xpzm4edp6o/gpt_ckpt.zip\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc2e80bd458e0ccc36f5f0ada2d9.dl.dropboxusercontent.com/cd/0/inline/BCdKDmTgEm3f-XkOreGeGt1oBrZt0rzfMZ0n_wU11LD246ByaqhXK1sPiLdoj_9xoxq7jcpWVWsErdpZGUGeTcs3p5vJ59stFMvMKTydBZayo25KS7xbIWXfQPZty34_xeo/file# [following]\n",
            "--2020-11-03 06:43:36--  https://uc2e80bd458e0ccc36f5f0ada2d9.dl.dropboxusercontent.com/cd/0/inline/BCdKDmTgEm3f-XkOreGeGt1oBrZt0rzfMZ0n_wU11LD246ByaqhXK1sPiLdoj_9xoxq7jcpWVWsErdpZGUGeTcs3p5vJ59stFMvMKTydBZayo25KS7xbIWXfQPZty34_xeo/file\n",
            "Resolving uc2e80bd458e0ccc36f5f0ada2d9.dl.dropboxusercontent.com (uc2e80bd458e0ccc36f5f0ada2d9.dl.dropboxusercontent.com)... 162.125.4.15, 2620:100:6018:15::a27d:30f\n",
            "Connecting to uc2e80bd458e0ccc36f5f0ada2d9.dl.dropboxusercontent.com (uc2e80bd458e0ccc36f5f0ada2d9.dl.dropboxusercontent.com)|162.125.4.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/BCcKw1COziZwLU7wbqDhO1W3Y-mwcLwPmJVpO5eFYAjshOH33MwOExT9PjaNitLDsOMKhyIupHKRFshnUAzCN7H9Bcf9phzQkxXE3meoKW2k219dHRonTr8cUWT4eaBd61-ADR_NGk48mpx4426431FxmPDiMZATCZM5ysHoa6j33zEyVrN8FUoXEBpWLqxnFfp73OF0m-4FOW19KyLRMV6WfGjvQEYpZfbaoQlK9S0_2KaWC4H6jJ_yvubQJqhXK07rASBk8PPgCMZtmUa7IOWSNnqb0roy3U_bN6RnjQstfn3tGzftOA_JDpsMCa-umFm2VjBXwINJdO8-oOHazUh_YnBueOSLNFveRAt0Q0FITg/file [following]\n",
            "--2020-11-03 06:43:37--  https://uc2e80bd458e0ccc36f5f0ada2d9.dl.dropboxusercontent.com/cd/0/inline2/BCcKw1COziZwLU7wbqDhO1W3Y-mwcLwPmJVpO5eFYAjshOH33MwOExT9PjaNitLDsOMKhyIupHKRFshnUAzCN7H9Bcf9phzQkxXE3meoKW2k219dHRonTr8cUWT4eaBd61-ADR_NGk48mpx4426431FxmPDiMZATCZM5ysHoa6j33zEyVrN8FUoXEBpWLqxnFfp73OF0m-4FOW19KyLRMV6WfGjvQEYpZfbaoQlK9S0_2KaWC4H6jJ_yvubQJqhXK07rASBk8PPgCMZtmUa7IOWSNnqb0roy3U_bN6RnjQstfn3tGzftOA_JDpsMCa-umFm2VjBXwINJdO8-oOHazUh_YnBueOSLNFveRAt0Q0FITg/file\n",
            "Reusing existing connection to uc2e80bd458e0ccc36f5f0ada2d9.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 460908853 (440M) [application/zip]\n",
            "Saving to: ‘gpt_ckpt.zip’\n",
            "\n",
            "gpt_ckpt.zip        100%[===================>] 439.56M  50.5MB/s    in 9.5s    \n",
            "\n",
            "2020-11-03 06:43:49 (46.1 MB/s) - ‘gpt_ckpt.zip’ saved [460908853/460908853]\n",
            "\n",
            "Archive:  gpt_ckpt.zip\n",
            "   creating: gpt_ckpt/\n",
            "  inflating: gpt_ckpt/gpt2_kor_tokenizer.spiece  \n",
            "  inflating: gpt_ckpt/config.json    \n",
            "  inflating: gpt_ckpt/tf_model.h5    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yz-KD16xduQ"
      },
      "source": [
        "# To define the GPT2Model class.\n",
        "class GPT2Model(tf.keras.Model):\n",
        "  def __init__(self, dir_path):\n",
        "    super(GPT2Model, self).__init__()\n",
        "    self.gpt2 = TFGPT2LMHeadModel.from_pretrained(dir_path)\n",
        "  \n",
        "  # object gpt2 includes 4 outputs of tuple (last_hidden_states, past, hidden_state, attentions)\n",
        "  def call(self, inputs):\n",
        "    return self.gpt2(inputs)[0]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ea5pRuNd2oag",
        "outputId": "242246bb-25e8-4360-c234-6df9e00a759d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# We used Colab.\n",
        "BASE_MODEL_PATH = '/content/drive/My Drive/ModelCollection/gpt_ckpt/'\n",
        "# Create pre-trained gpt2 model.\n",
        "gpt_model = GPT2Model(BASE_MODEL_PATH)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at /content/drive/My Drive/ModelCollection/gpt_ckpt/.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9tEvF8x3P2f"
      },
      "source": [
        "## Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdSZelX224q8"
      },
      "source": [
        "TOKENIZER_PATH = os.path.join(BASE_MODEL_PATH + 'gpt2_kor_tokenizer.spiece')\n",
        "\n",
        "# To make tokenizer for KoGPT2.\n",
        "tokenizer = SentencepieceTokenizer(TOKENIZER_PATH)\n",
        "\n",
        "# To define the word dictionary.\n",
        "vocab = nlp.vocab.BERTVocab.from_sentencepiece(TOKENIZER_PATH,\n",
        "                                               mask_token=None,\n",
        "                                               sep_token=None,\n",
        "                                               cls_token=None,\n",
        "                                               unknown_token='<unk>',\n",
        "                                               padding_token='<pad>',\n",
        "                                               bos_token='<s>',\n",
        "                                               eos_token='</s>')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-EshctH4U9A"
      },
      "source": [
        "## Sentence Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2osQt10N3i_7"
      },
      "source": [
        "def tf_top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-99999):\n",
        "    _logits = logits.numpy()\n",
        "    top_k = min(top_k, logits.shape[-1])  \n",
        "    \n",
        "    # top-k sampling method\n",
        "    if top_k > 0:\n",
        "        indices_to_remove = logits < tf.math.top_k(logits, top_k)[0][..., -1, None]\n",
        "        _logits[indices_to_remove] = filter_value\n",
        "    \n",
        "    # Nuclus sampling method\n",
        "    if top_p > 0.0:\n",
        "        sorted_logits = tf.sort(logits, direction='DESCENDING')\n",
        "        sorted_indices = tf.argsort(logits, direction='DESCENDING')\n",
        "        cumulative_probs = tf.math.cumsum(tf.nn.softmax(sorted_logits, axis=-1), axis=-1)\n",
        "\n",
        "        sorted_indices_to_remove = cumulative_probs > top_p\n",
        "        sorted_indices_to_remove = tf.concat([[False], sorted_indices_to_remove[..., :-1]], axis=0)\n",
        "        indices_to_remove = sorted_indices[sorted_indices_to_remove].numpy().tolist()\n",
        "        \n",
        "        _logits[indices_to_remove] = filter_value\n",
        "    return tf.constant([_logits])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2_17rB94X82"
      },
      "source": [
        "def generate_sent(seed_word, model, max_step=100, greedy=False, top_k=0, top_p=0.):\n",
        "    sent = seed_word   # Input sentence or word\n",
        "    toked = tokenizer(sent)  # Tokenizing\n",
        "    \n",
        "    for _ in range(max_step):  # max_step is the maximum size of sentence generating\n",
        "        input_ids = tf.constant([vocab[vocab.bos_token],]  + vocab[toked])[None, :] \n",
        "        outputs = model(input_ids)[:, -1, :] # Output is the last subword of sentence.\n",
        "        if greedy:  # greedy search with Maximum Likelihood Estimation.\n",
        "            gen = vocab.to_tokens(tf.argmax(outputs, axis=-1).numpy().tolist()[0])\n",
        "        else:    # Using top_k & Nucleus sampling.\n",
        "            output_logit = tf_top_k_top_p_filtering(outputs[0], top_k=top_k, top_p=top_p)\n",
        "            gen = vocab.to_tokens(tf.random.categorical(output_logit, 1).numpy().tolist()[0])[0]\n",
        "        if gen == '</s>': # Stop generating when meeting this special token.\n",
        "            break\n",
        "        sent += gen.replace('▁', ' ')\n",
        "        toked = tokenizer(sent)\n",
        "\n",
        "    return sent"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cynj79PC4Z9a"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}