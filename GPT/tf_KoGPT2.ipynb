{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf_KoGPT2.ipynb",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "https://github.com/HyeonhoonLee/KoNLP/blob/master/GPT/tf_KoGPT2.ipynb",
      "authorship_tag": "ABX9TyOnj9T/0y2oyn+ApbP9eeXq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HyeonhoonLee/KoNLP/blob/master/GPT/tf_KoGPT2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZcQqI2rpoTQ"
      },
      "source": [
        "# Korean Language Modeling using GPT2\n",
        "\n",
        "- The contents of this notebook is modified from [NLP-Kr](https://github.com/NLP-kr/tensorflow-ml-nlp-tf2)\n",
        "- The source codes are based on [HuggingFace Transformers](https://github.com/huggingface/transformers) and [SKT KoGPT2](https://github.com/SKT-AI/KoGPT2)\n",
        "- To understand the pricinpale of GPT2, see this kindful page of [illustrated GPT-2](https://jalammar.github.io/illustrated-gpt2/).\n",
        "- The codes below use Tensorflow 2.3.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bR1lM7RrNeU"
      },
      "source": [
        "##Libraries and modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpaMiYBExfq7"
      },
      "source": [
        "!pip install gluonnlp\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yu2u-6RTxtRZ"
      },
      "source": [
        "!pip install mxnet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ITq77Fs1kXe"
      },
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPBDCbMAoEtE"
      },
      "source": [
        "# To use the word vocab API (https://nlp.gluon.ai/api/vocab.html).\n",
        "import gluonnlp as nlp  \n",
        "\n",
        "# Do not use the tokenizer of Transformers. KoGPT2 model used this type of Tokenizer.\n",
        "# This tokenizer supports subword tokenization such as BPE(Byte pair Encoding).\n",
        "# (https://nlp.gluon.ai/api/data.html)\n",
        "from gluonnlp.data import SentencepieceTokenizer\n",
        "\n",
        "# To use Open AI GPT2 Model transformer with a language modeling head on top (linear layer with weights tied to the input embeddings).\n",
        "# (https://huggingface.co/transformers/model_doc/gpt2.html#gpt2lmheadmodel)\n",
        "from transformers import TFGPT2LMHeadModel"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLr1_hb502vN"
      },
      "source": [
        "##Korean GPT2 Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-F_KB-h2LaQ"
      },
      "source": [
        "# # Download the pretrained parameters.\n",
        "# !wget https://www.dropbox.com/s/nzfa9xpzm4edp6o/gpt_ckpt.zip -O gpt_ckpt.zip\n",
        "# !unzip -o gpt_ckpt.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yz-KD16xduQ"
      },
      "source": [
        "# To define the GPT2Model class.\n",
        "class GPT2Model(tf.keras.Model):\n",
        "  def __init__(self, dir_path):\n",
        "    super(GPT2Model, self).__init__()\n",
        "    self.gpt2 = TFGPT2LMHeadModel.from_pretrained(dir_path)\n",
        "  \n",
        "  # object gpt2 includes 4 outputs of tuple (last_hidden_states, past, hidden_state, attentions)\n",
        "  def call(self, inputs):\n",
        "    return self.gpt2(inputs)[0]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ea5pRuNd2oag",
        "outputId": "ec024061-fdea-47af-ea65-2d8761f7cac2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# We used Colab.\n",
        "BASE_MODEL_PATH = '/content/drive/My Drive/ModelCollection/gpt_ckpt/'\n",
        "# Create pre-trained gpt2 model.\n",
        "gpt_model = GPT2Model(BASE_MODEL_PATH)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at /content/drive/My Drive/ModelCollection/gpt_ckpt/.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9tEvF8x3P2f"
      },
      "source": [
        "## Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdSZelX224q8"
      },
      "source": [
        "TOKENIZER_PATH = os.path.join(BASE_MODEL_PATH + 'gpt2_kor_tokenizer.spiece')\n",
        "\n",
        "# To make tokenizer for KoGPT2.\n",
        "tokenizer = SentencepieceTokenizer(TOKENIZER_PATH)\n",
        "\n",
        "# To define the word dictionary.\n",
        "vocab = nlp.vocab.BERTVocab.from_sentencepiece(TOKENIZER_PATH,\n",
        "                                               mask_token=None,\n",
        "                                               sep_token=None,\n",
        "                                               cls_token=None,\n",
        "                                               unknown_token='<unk>',\n",
        "                                               padding_token='<pad>',\n",
        "                                               bos_token='<s>',\n",
        "                                               eos_token='</s>')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-EshctH4U9A"
      },
      "source": [
        "## Sentence Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2osQt10N3i_7"
      },
      "source": [
        "def tf_top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-99999):\n",
        "    _logits = logits.numpy()\n",
        "    top_k = min(top_k, logits.shape[-1])  \n",
        "    \n",
        "    # top-k sampling method\n",
        "    if top_k > 0:\n",
        "        indices_to_remove = logits < tf.math.top_k(logits, top_k)[0][..., -1, None]\n",
        "        _logits[indices_to_remove] = filter_value\n",
        "    \n",
        "    # Nuclus sampling method\n",
        "    if top_p > 0.0:\n",
        "        sorted_logits = tf.sort(logits, direction='DESCENDING')\n",
        "        sorted_indices = tf.argsort(logits, direction='DESCENDING')\n",
        "        cumulative_probs = tf.math.cumsum(tf.nn.softmax(sorted_logits, axis=-1), axis=-1)\n",
        "\n",
        "        sorted_indices_to_remove = cumulative_probs > top_p\n",
        "        sorted_indices_to_remove = tf.concat([[False], sorted_indices_to_remove[..., :-1]], axis=0)\n",
        "        indices_to_remove = sorted_indices[sorted_indices_to_remove].numpy().tolist()\n",
        "        \n",
        "        _logits[indices_to_remove] = filter_value\n",
        "    return tf.constant([_logits])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2_17rB94X82"
      },
      "source": [
        "def generate_sent(seed_word, model, max_step=100, greedy=False, top_k=0, top_p=0.):\n",
        "    sent = seed_word   # Input sentence or word\n",
        "    toked = tokenizer(sent)  # Tokenizing\n",
        "    \n",
        "    for _ in range(max_step):  # max_step is the maximum size of sentence generating\n",
        "        input_ids = tf.constant([vocab[vocab.bos_token],]  + vocab[toked])[None, :] \n",
        "        outputs = model(input_ids)[:, -1, :] # Output is the last subword of sentence.\n",
        "        if greedy:  # greedy search with Maximum Likelihood Estimation.\n",
        "            gen = vocab.to_tokens(tf.argmax(outputs, axis=-1).numpy().tolist()[0])\n",
        "        else:    # Using top_k & Nucleus sampling.\n",
        "            output_logit = tf_top_k_top_p_filtering(outputs[0], top_k=top_k, top_p=top_p)\n",
        "            gen = vocab.to_tokens(tf.random.categorical(output_logit, 1).numpy().tolist()[0])[0]\n",
        "        if gen == '</s>': # Stop generating when meeting this special token.\n",
        "            break\n",
        "        sent += gen.replace('‚ñÅ', ' ')\n",
        "        toked = tokenizer(sent)\n",
        "\n",
        "    return sent"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KomPy7uX71az"
      },
      "source": [
        "## Practice of sentence generating"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cynj79PC4Z9a",
        "outputId": "2b1931e5-6502-4a4d-a51f-d5088721c4d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "generate_sent('ÏùòÌïô', gpt_model, greedy=True)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'ÏùòÌïôÏ†ÑÎ¨∏ÎåÄÌïôÏõê, ÏùòÍ≥ºÎåÄÌïôÏõêÍ≥º Í∞ôÏùÄ ÌïôÏõêÏùÑ ÏÑ§Î¶ΩÌï† Ïàò ÏûàÎäîÍ∞ÄÏóê ÎåÄÌïú Í≤ÉÏù¥Îã§.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRx1WS-27-2_",
        "outputId": "439271b5-3a4c-4b17-a8a3-27f0818a19e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "generate_sent('ÏùòÌïô', gpt_model, top_k=0, top_p=0.95)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'ÏùòÌïô, Ìù°Ïó∞, Ïã†Ïû• Ïó∞Íµ¨ÏùòÌïôÏùÑ Ï†ÑÎ¨∏Ï†ÅÏúºÎ°ú ÌïòÎ©¥ÏÑúÏù¥ÏßÄ.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRDHwwKq8SiP"
      },
      "source": [
        "## Fine tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Blt7oSWJ8Voj"
      },
      "source": [
        "### Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THXRFQqC8TiF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-W3KCJ_8XsP"
      },
      "source": [
        "### Fine tuning with pre-trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zj2GSKbF8cTA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}