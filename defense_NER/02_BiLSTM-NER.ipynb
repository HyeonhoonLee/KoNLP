{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "data_dir = os.path.join(os.getcwd(), 'data')\n",
    "\n",
    "print (tf.__version__)\n",
    "print (data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ner_data(data_dir:str, file_name:str):\n",
    "    with open(os.path.join(data_dir, file_name), mode = 'r', encoding= 'utf-8') as f:\n",
    "        data = [line.strip().splitlines() for line in f.read().split('\\n=====\\n') if line.strip()]\n",
    "        data = [[tuple(tk.split('\\t')) for tk in line] for line in data]\n",
    "    return data\n",
    "\n",
    "%time train_sents = list(read_ner_data(data_dir, 'train.txt'))\n",
    "print ('n_Train Data set: %d\\n'%len(train_sents))\n",
    "\n",
    "%time test_sents = list(read_ner_data(data_dir, 'test.txt'))\n",
    "print ('n_Test Data set: %d\\n'%len(test_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint \n",
    "\n",
    "line = train_sents[0]\n",
    "pprint (line)\n",
    "print ('\\n==================\\n')\n",
    "\n",
    "sentence = np.array(line)[:,0].tolist()\n",
    "bio_tags = np.array(line)[:,2].tolist()\n",
    "print (sentence, '\\n', bio_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [np.array(line)[:,0].tolist() for line in tqdm(train_sents)]\n",
    "bio_tags = [np.array(line)[:,2].tolist() for line in tqdm(train_sents)]\n",
    "\n",
    "print ('\\n')\n",
    "print (sentences[0])\n",
    "print (bio_tags[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('샘플의 최대 길이 : %d' % max(len(l) for l in sentences))\n",
    "print('샘플의 평균 길이 : %f' % (sum(map(len, sentences))/len(sentences)))\n",
    "plt.hist([len(s) for s in sentences], bins=50)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokenizer = Tokenizer(oov_token='OOV') # 모든 단어를 사용하지만 인덱스 1에는 단어 'OOV'를 할당한다.\n",
    "input_tokenizer.fit_on_texts(sentences)\n",
    "output_tokenizer = Tokenizer(lower=False) # 태깅 정보들은 내부적으로 대문자를 유지한채로 저장\n",
    "output_tokenizer.fit_on_texts(bio_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer를 통해, 데이터 인코딩\n",
    "X_train = input_tokenizer.texts_to_sequences(sentences)\n",
    "y_train = output_tokenizer.texts_to_sequences(bio_tags)\n",
    "\n",
    "print ('원본   데이터  : ', sentences[0])\n",
    "print ('인코딩   데이터: ', X_train[0], '\\n')\n",
    "print ('원본   BIO Tags: ', bio_tags[0])\n",
    "print ('인코딩 BIO Tags: ', y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = input_tokenizer.word_index\n",
    "index2word = input_tokenizer.index_word\n",
    "tags2index = output_tokenizer.word_index\n",
    "index2tags = output_tokenizer.index_word\n",
    "index2tags[0] = 'PAD'\n",
    "\n",
    "index2tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(input_tokenizer.word_index) + 1\n",
    "tag_size = len(output_tokenizer.word_index) + 1\n",
    "print('단어 집합의 크기 : {}'.format(vocab_size))\n",
    "print('개체명 태깅 정보 집합의 크기 : {}'.format(tag_size))\n",
    "print('단어 OOV의 인덱스 : {}'.format(input_tokenizer.word_index['OOV']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 75\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=max_seq_len)\n",
    "y_train = pad_sequences(y_train, padding='post', maxlen=max_seq_len)\n",
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data 준비\n",
    "X_test = [np.array(line)[:,0].tolist() for line in tqdm(test_sents)]\n",
    "X_test = input_tokenizer.texts_to_sequences(X_test)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=max_seq_len)\n",
    "\n",
    "y_test = [np.array(line)[:,2].tolist() for line in tqdm(test_sents)]\n",
    "y_test = output_tokenizer.texts_to_sequences(y_test)\n",
    "y_test = pad_sequences(y_test, padding='post', maxlen=max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train, num_classes=tag_size)\n",
    "y_test = to_categorical(y_test, num_classes=tag_size)\n",
    "\n",
    "print('훈련 샘플 문장의 크기 : {}'.format(X_train.shape))\n",
    "print('훈련 샘플 레이블의 크기 : {}'.format(y_train.shape))\n",
    "print('테스트 샘플 문장의 크기 : {}'.format(X_test.shape))\n",
    "print('테스트 샘플 레이블의 크기 : {}'.format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Bidirectional, TimeDistributed, Embedding\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 128, input_length=max_seq_len, mask_zero=True))\n",
    "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
    "model.add(TimeDistributed(Dense(tag_size, activation='softmax')))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(0.001), metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=128, epochs=3,  validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_crfsuite import scorers, metrics\n",
    "\n",
    "labels = list(index2tags.values())\n",
    "labels.remove('O')\n",
    "labels.remove('PAD')\n",
    "\n",
    "true_y = np.argmax(y_test, axis=-1)\n",
    "true_y = [[index2tags[idx] for idx in true_tags if idx != 0] for true_tags in tqdm(true_y)]\n",
    "\n",
    "prd_y = model.predict(np.array(X_test))\n",
    "prd_y = np.argmax(prd_y, axis=-1)\n",
    "prd_y = [np.array(list(zip(true_y[i], prd_y[i])))[:,1].tolist() \n",
    "    for i in range(len(prd_y))]\n",
    "prd_y = [[index2tags[int(idx)] for idx in prd_tags] for prd_tags in tqdm(prd_y)]\n",
    "\n",
    "\n",
    "f1_score = metrics.flat_f1_score(true_y, prd_y, average='weighted', labels=labels)\n",
    "print ('\\n\\n========= Model Validataion =============\\n')\n",
    "print ('F1 score: %0.3f'%f1_score)\n",
    "\n",
    "sorted_labels = sorted(\n",
    "    labels, \n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")\n",
    "\n",
    "print (metrics.flat_classification_report(\n",
    "    true_y, \n",
    "    prd_y, \n",
    "    labels=sorted_labels, \n",
    "    digits=3\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
